#!/usr/bin/env python3
"""
üöÄ FULL COMPLETE PIPELINE - Multi-Layer Analytics
All-in-one script: YouTube Scraping ‚Üí Preprocessing ‚Üí Training ‚Üí Analysis ‚Üí Visualization

Features:
- YouTube Data scraping (requires API key or uses fallback)
- Complete text preprocessing
- Multi-layer sentiment analysis
- Comprehensive reporting

Run: python FULL_PIPELINE.py
"""

import pandas as pd
import numpy as np
import re
import os
import json
import yaml
from pathlib import Path
from datetime import datetime
from collections import Counter

# Try to import YouTube API
try:
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
    YOUTUBE_API_AVAILABLE = True
except ImportError:
    YOUTUBE_API_AVAILABLE = False
    print("‚ö†Ô∏è  Google API client not installed. Using fallback data generation.")

print("="*100)
print("üöÄ FULL COMPLETE MULTI-LAYER ANALYTICS PIPELINE")
print("="*100)
print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*100)

# ============================================================================
# STEP 1: DATA COLLECTION (YouTube Scraping or Fallback)
# ============================================================================
print("\nüìä STEP 1: Data collection...")

# Load config
try:
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    API_KEY = config['api_keys']['youtube']
    KEYWORDS = config['search_keywords']
    MAX_VIDEOS = config['data_collection']['max_videos_per_query']
    MAX_COMMENTS = config['data_collection']['max_comments_per_video']
    print(f"‚úÖ Config loaded: {len(KEYWORDS)} keywords, max {MAX_VIDEOS} videos, {MAX_COMMENTS} comments each")
except:
    API_KEY = "YOUR_YOUTUBE_API_KEY"
    KEYWORDS = ["Timnas Indonesia", "Piala Dunia Indonesia", "Sepakbola Indonesia"]
    MAX_VIDEOS = 5
    MAX_COMMENTS = 100
    print("‚ö†Ô∏è  Config not found, using defaults")

def scrape_youtube_data(api_key, keywords, max_videos=5, max_comments=100):
    """
    Scrape YouTube comments using official API
    """
    if not YOUTUBE_API_AVAILABLE:
        return None, None
    
    if api_key == "YOUR_YOUTUBE_API_KEY":
        print("‚ö†Ô∏è  API key not configured, skipping real scraping")
        return None, None
    
    try:
        youtube = build('youtube', 'v3', developerKey=api_key)
        all_videos = []
        all_comments = []
        
        for keyword in keywords:
            print(f"\n   üîç Searching: '{keyword}'")
            
            # Search videos
            search_response = youtube.search().list(
                q=keyword,
                part='id,snippet',
                type='video',
                maxResults=max_videos,
                order='relevance',
                regionCode='ID'
            ).execute()
            
            videos = search_response.get('items', [])
            print(f"      Found {len(videos)} videos")
            
            for video in videos:
                video_id = video['id']['videoId']
                video_title = video['snippet']['title']
                
                # Get video stats
                video_stats = youtube.videos().list(
                    part='statistics',
                    id=video_id
                ).execute()
                
                stats = video_stats['items'][0]['statistics'] if video_stats['items'] else {}
                
                all_videos.append({
                    'video_id': video_id,
                    'title': video_title,
                    'description': video['snippet']['description'],
                    'channel': video['snippet']['channelTitle'],
                    'published_at': video['snippet']['publishedAt'],
                    'view_count': stats.get('viewCount', 0),
                    'like_count': stats.get('likeCount', 0),
                    'comment_count': stats.get('commentCount', 0),
                    'keyword': keyword
                })
                
                # Get comments
                print(f"      üìù Fetching comments for: {video_title[:40]}...")
                try:
                    comments_response = youtube.commentThreads().list(
                        part='snippet',
                        videoId=video_id,
                        maxResults=max_comments,
                        textFormat='plainText',
                        order='relevance'
                    ).execute()
                    
                    for comment in comments_response.get('items', []):
                        snippet = comment['snippet']['topLevelComment']['snippet']
                        all_comments.append({
                            'video_id': video_id,
                            'video_title': video_title,
                            'comment_id': comment['id'],
                            'text': snippet['textDisplay'],
                            'author': snippet['authorDisplayName'],
                            'likes': snippet['likeCount'],
                            'published_at': snippet['publishedAt'],
                            'keyword': keyword
                        })
                    
                    print(f"         Got {len(comments_response.get('items', []))} comments")
                
                except HttpError as e:
                    if 'commentsDisabled' in str(e):
                        print(f"         ‚ö†Ô∏è  Comments disabled")
                    else:
                        print(f"         ‚ùå Error: {e}")
        
        return pd.DataFrame(all_videos), pd.DataFrame(all_comments)
    
    except Exception as e:
        print(f"‚ùå Scraping error: {e}")
        return None, None

def generate_extended_dataset(size='large'):
    """
    Generate comprehensive synthetic dataset when scraping unavailable
    Size: 'small' (100), 'medium' (300), 'large' (500+)
    """
    print(f"   üìù Generating {size} synthetic dataset...")
    
    # Base comment templates with variations
    templates = {
        'positif_bangga': [
            "Indonesia memang hebat! Bangga sama timnas!",
            "Luar biasa perjuangan anak bangsa! Salut!",
            "Garuda di dada saya, bangga jadi Indonesia!",
            "Bangga dengan semangat juang para pemain!",
            "Prestasi timnas semakin membaik, bangga!",
            "Hebat! Performa timnas sangat bagus!",
            "Salut dedikasi pemain timnas Indonesia!",
            "Pemain muda Indonesia punya mental juara!",
            "Kerja keras pemain timnas patut diapresiasi!",
            "Indonesia bisa! Timnas membanggakan!",
        ],
        'positif_senang': [
            "Tetap semangat dan dukung timnas!",
            "Ayo terus dukung timnas Indonesia!",
            "Senang lihat perkembangan timnas!",
            "Tetap semangat boys! Kalian bisa!",
            "Yuk dukung garuda di lapangan!",
            "Senang lihat pemain muda berkembang!",
            "Dukungan fans luar biasa!",
            "Mari dukung bersama timnas!",
            "Semangat terus timnas Indonesia!",
            "Bagus! Terus tingkatkan performa!",
        ],
        'negatif_kecewa': [
            "Kecewa banget sama timnas, performa buruk",
            "Sangat mengecewakan hasil pertandingan ini",
            "Kecewa dengan performa pemain",
            "Timnas harus evaluasi total, mengecewakan",
            "Kecewa berat dengan kualifikasi ini",
            "Harapan tinggi tapi kenyataan mengecewakan",
            "Performa buruk, sangat kecewa!",
            "Kecewa dengan timnas, kenapa selalu begini?",
            "Mengecewakan! Pemain tidak maksimal",
            "Kecewa total, banyak kesempatan terbuang",
        ],
        'negatif_marah_toxic': [
            "PSSI bodoh! Manajemen berantakan!",
            "Pelatih payah! Strategi kacau balau!",
            "PSSI sampah! Korupsi terus!",
            "Bodoh semua! Ga ada yang bener!",
            "Tolol! Kenapa pemain bagus di bench?",
        ],
        'negatif_komplain': [
            "Pelatih harus diganti, tidak cocok",
            "Strategi terlalu defensif, kurang menyerang",
            "Strategi pelatih sangat buruk",
            "Pelatih harus baca situasi lebih baik",
            "Taktik tidak efektif, perlu perubahan",
            "Formasi tidak cocok untuk pemain kita",
            "Pelatih kurang fleksibel ubah strategi",
            "Strategi terlalu konservatif",
        ],
        'negatif_saran': [
            "Timnas harus evaluasi dan belajar",
            "Perlu rotasi pemain yang lebih baik",
            "Komunikasi antar pemain harus ditingkatkan",
            "PSSI fokus pembinaan pemain muda",
            "Evaluasi menyeluruh diperlukan",
            "Perlu peningkatan kualitas latihan",
            "Manajemen harus lebih transparan",
            "Perlu program jangka panjang",
        ],
        'netral_informasi': [
            "Namanya juga sepakbola ada menang ada kalah",
            "Kualifikasi piala dunia memang sulit",
            "Pemain muda Indonesia punya potensi",
            "Strategi menyerang lebih bagus",
            "Pelatih perlu waktu pahami pemain",
            "Sepakbola Indonesia dalam masa transisi",
            "Kompetisi di Asia sangat ketat",
            "Perlu kesabaran bangun tim solid",
        ],
        'pertanyaan': [
            "Kapan timnas bisa masuk piala dunia?",
            "Kenapa strategi tidak berubah?",
            "Apakah akan ada pergantian pelatih?",
            "Siapa pemain terbaik Indonesia?",
            "Bagaimana PSSI perbaiki sistem?",
            "Mengapa pemain naturalisasi tidak main?",
        ],
        'wasit': [
            "Wasit memihak! Harusnya dapat penalty!",
            "Keputusan wasit merugikan timnas",
            "Wasit tidak profesional",
            "Banyak pelanggaran tidak dikasih kartu",
            "VAR tidak digunakan dengan benar",
        ],
        'fans': [
            "Fans Indonesia selalu setia mendukung",
            "Supporter Indonesia luar biasa ramai",
            "Dukungan fans bantu semangat pemain",
            "Bangga jadi pendukung timnas",
            "Suporter Indonesia terbaik se-Asia!",
        ],
    }
    
    # Determine size
    size_map = {'small': 100, 'medium': 300, 'large': 500}
    target_size = size_map.get(size, 500)
    
    # Generate comments with variations
    comments = []
    likes_range = {'positif': (100, 500), 'negatif': (20, 200), 'netral': (50, 150)}
    
    while len(comments) < target_size:
        for category, texts in templates.items():
            for text in texts:
                if len(comments) >= target_size:
                    break
                
                # Add variations
                variations = [
                    text,
                    text + " üëç",
                    text + " üáÆüá©",
                    text.replace("timnas", "Timnas Indonesia"),
                    text.replace("pemain", "para pemain"),
                ]
                
                for var_text in variations[:2]:  # Use 2 variations per template
                    if len(comments) >= target_size:
                        break
                    
                    # Determine sentiment
                    if 'positif' in category:
                        sentiment = 'positif'
                    elif 'negatif' in category:
                        sentiment = 'negatif'
                    else:
                        sentiment = 'netral'
                    
                    like_min, like_max = likes_range[sentiment]
                    
                    comments.append({
                        'text': var_text,
                        'likes': np.random.randint(like_min, like_max),
                        'category': category
                    })
    
    # Create DataFrame
    df = pd.DataFrame(comments[:target_size])
    df['video_id'] = ['VID_' + str(i // 50 + 1) for i in range(len(df))]
    df['video_title'] = ['Timnas Indonesia - Match ' + str(i // 50 + 1) for i in range(len(df))]
    df['comment_id'] = ['CMT_' + str(i).zfill(5) for i in range(len(df))]
    df['author'] = ['User' + str(np.random.randint(1, 200)) for _ in range(len(df))]
    df['published_at'] = '2024-11-01T12:00:00Z'
    df['keyword'] = 'Timnas Indonesia'
    
    videos_count = len(df['video_id'].unique())
    videos_df = pd.DataFrame({
        'video_id': df['video_id'].unique(),
        'title': ['Timnas Indonesia - Match ' + str(i+1) for i in range(videos_count)],
        'description': ['Pertandingan Timnas Indonesia'] * videos_count,
        'channel': ['Football Indonesia Channel'] * videos_count,
        'published_at': ['2024-11-01T12:00:00Z'] * videos_count,
        'view_count': np.random.randint(10000, 100000, videos_count),
        'like_count': np.random.randint(500, 5000, videos_count),
        'comment_count': [50] * videos_count,
        'keyword': ['Timnas Indonesia'] * videos_count
    })
    
    return videos_df, df

# Try scraping first, fallback to synthetic data
videos_df, comments_df = scrape_youtube_data(API_KEY, KEYWORDS, MAX_VIDEOS, MAX_COMMENTS)

if comments_df is None or len(comments_df) == 0:
    print("\n   ‚ö†Ô∏è  Scraping unavailable, using synthetic data...")
    videos_df, comments_df = generate_extended_dataset('large')
    print(f"   ‚úÖ Synthetic dataset generated: {len(comments_df)} comments")
else:
    # Rename columns to match
    if 'likes' not in comments_df.columns and 'like_count' in comments_df.columns:
        comments_df.rename(columns={'like_count': 'likes'}, inplace=True)
    print(f"   ‚úÖ Real data scraped: {len(comments_df)} comments")

df = comments_df.copy()

print(f"‚úÖ Data collected: {len(df)} comments")
print(f"   Across {len(df['video_id'].unique())} videos")

# Save raw data
os.makedirs('data/raw', exist_ok=True)
df.to_csv('data/raw/comments_full.csv', index=False, encoding='utf-8')
videos_df.to_csv('data/raw/videos_full.csv', index=False, encoding='utf-8')
print(f"   üíæ Saved: data/raw/comments_full.csv, videos_full.csv")

# ============================================================================
# STEP 2: TEXT PREPROCESSING (Built-in, no Sastrawi needed)
# ============================================================================
print("\nüîß STEP 2: Preprocessing text...")

# Indonesian stopwords (common ones)
STOPWORDS = set([
    'yang', 'di', 'ke', 'dari', 'untuk', 'pada', 'dengan', 'adalah', 'ini', 'itu',
    'dan', 'atau', 'juga', 'akan', 'telah', 'dapat', 'tidak', 'ada', 'dalam', 'oleh',
    'ia', 'dia', 'mereka', 'kita', 'kami', 'saya', 'aku', 'nya', 'mu', 'ku',
    'sudah', 'belum', 'masih', 'sangat', 'lebih', 'paling', 'se', 'ter', 'para',
    'bisa', 'harus', 'ingin', 'mau', 'perlu', 'banyak', 'sedikit', 'semua', 'setiap',
])

# Slang normalization
SLANG_DICT = {
    'gk': 'tidak', 'ga': 'tidak', 'tdk': 'tidak', 'gak': 'tidak',
    'udh': 'sudah', 'udah': 'sudah', 'dah': 'sudah',
    'yg': 'yang', 'yng': 'yang',
    'bgus': 'bagus', 'bgs': 'bagus',
    'bgt': 'banget', 'bener': 'benar',
    'org': 'orang', 'orng': 'orang',
    'dgn': 'dengan', 'dg': 'dengan',
    'utk': 'untuk', 'untk': 'untuk',
    'kmrn': 'kemarin', 'kmren': 'kemarin',
    'skrg': 'sekarang', 'skrang': 'sekarang',
    'tp': 'tapi', 'tetap': 'tetap',
    'wkwk': 'haha', 'wkwkwk': 'haha', 'kwkw': 'haha',
    'gpp': 'tidak apa apa',
}

# Sentiment keywords
POSITIVE_WORDS = {
    'hebat', 'bagus', 'menang', 'bangga', 'senang', 'semangat', 'luar biasa', 
    'salut', 'prestasi', 'baik', 'mantap', 'keren', 'oke', 'solid', 'kompak',
    'juara', 'sukses', 'berhasil', 'membanggakan', 'apresiasi', 'positif',
    'maju', 'berkembang', 'dedikasi', 'kerja keras', 'perjuangan', 'mental',
}

NEGATIVE_WORDS = {
    'buruk', 'kecewa', 'gagal', 'kalah', 'bodoh', 'payah', 'berantakan',
    'mengecewakan', 'jelek', 'hancur', 'amburadul', 'kacau', 'tolol',
    'sampah', 'korupsi', 'terbuang', 'pupus', 'frustrasi', 'lemah',
    'tidak', 'kurang', 'lemah', 'gagal', 'menyerah',
}

def clean_text(text):
    """Clean and normalize text"""
    text = str(text).lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#', '', text)
    # Keep punctuation for question detection
    text = re.sub(r'[^\w\s\?\!]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def normalize_slang(text):
    """Normalize Indonesian slang"""
    words = text.split()
    normalized = [SLANG_DICT.get(w, w) for w in words]
    return ' '.join(normalized)

def remove_stopwords(text):
    """Remove Indonesian stopwords"""
    words = text.split()
    filtered = [w for w in words if w not in STOPWORDS and len(w) > 2]
    return ' '.join(filtered)

def preprocess(text):
    """Complete preprocessing pipeline"""
    text = clean_text(text)
    text = normalize_slang(text)
    text = remove_stopwords(text)
    return text

# Apply preprocessing
df['text_cleaned'] = df['text'].apply(preprocess)
df['text_length'] = df['text'].apply(len)
df['word_count'] = df['text_cleaned'].apply(lambda x: len(x.split()))

print(f"‚úÖ Preprocessing completed")
print(f"   Average text length: {df['text_length'].mean():.1f} chars")
print(f"   Average word count: {df['word_count'].mean():.1f} words")

# ============================================================================
# STEP 3: RULE-BASED SENTIMENT ANALYSIS
# ============================================================================
print("\nüé≠ STEP 3: Sentiment analysis...")

def calculate_sentiment(text, original_text):
    """Calculate sentiment score"""
    words = set(text.lower().split())
    
    pos_count = sum(1 for w in words if w in POSITIVE_WORDS)
    neg_count = sum(1 for w in words if w in NEGATIVE_WORDS)
    
    # Bonus for original capitalized words (emphasis)
    if original_text.isupper():
        neg_count += 1
    
    total = pos_count + neg_count
    if total == 0:
        return 0.0, 'netral'
    
    score = (pos_count - neg_count) / (pos_count + neg_count + 1)
    
    if score > 0.2:
        return score, 'positif'
    elif score < -0.2:
        return score, 'negatif'
    else:
        return score, 'netral'

df['sentiment_score'], df['sentiment'] = zip(*df.apply(
    lambda row: calculate_sentiment(row['text_cleaned'], row['text']), axis=1
))
df['sentiment_confidence'] = df['sentiment_score'].abs()

print(f"‚úÖ Sentiment classified")
print(f"\nDistribution:")
for sent in ['positif', 'netral', 'negatif']:
    count = (df['sentiment'] == sent).sum()
    pct = count / len(df) * 100
    print(f"   {sent.capitalize()}: {count} ({pct:.1f}%)")

# ============================================================================
# STEP 4: MULTI-LAYER LABELING (Rule-Based)
# ============================================================================
print("\nüè∑Ô∏è STEP 4: Generating multi-layer labels...")

# EMOTION labeling
def label_emotions(text, sentiment):
    """Multi-label emotion classification"""
    text_lower = text.lower()
    emotions = []
    
    if sentiment == 'negatif':
        if any(w in text_lower for w in ['kecewa', 'mengecewakan', 'harapan', 'pupus']):
            emotions.append('kecewa')
        if any(w in text_lower for w in ['bodoh', 'payah', 'tolol', 'sampah', 'berantakan']):
            emotions.append('marah')
        if any(w in text_lower for w in ['sedih', 'menyedih', 'duka']):
            emotions.append('sedih')
        if any(w in text_lower for w in ['takut', 'khawatir', 'cemas']):
            emotions.append('takut')
    elif sentiment == 'positif':
        if any(w in text_lower for w in ['bangga', 'membanggakan', 'garuda', 'salut', 'apresiasi']):
            emotions.append('bangga')
        if any(w in text_lower for w in ['senang', 'semangat', 'ayo', 'bagus', 'hebat']):
            emotions.append('senang')
    
    return emotions if emotions else ['senang' if sentiment == 'positif' else 'kecewa']

df['emotions'] = df.apply(lambda row: label_emotions(row['text'], row['sentiment']), axis=1)

# ASPECT labeling
def label_aspects(text):
    """Multi-label aspect classification"""
    text_lower = text.lower()
    aspects = []
    
    if any(w in text_lower for w in ['manajemen', 'pssi', 'federasi', 'organisasi', 'sistem', 'korupsi']):
        aspects.append('manajemen')
    if any(w in text_lower for w in ['pelatih', 'shin', 'tae-yong', 'coach', 'taktik pelatih']):
        aspects.append('pelatih')
    if any(w in text_lower for w in ['pemain', 'timnas', 'anak bangsa', 'skuad', 'boys', 'mental']):
        aspects.append('pemain')
    if any(w in text_lower for w in ['strategi', 'defensif', 'menyerang', 'taktik', 'formasi']):
        aspects.append('strategi')
    if any(w in text_lower for w in ['wasit', 'referee', 'penalty', 'var', 'keputusan', 'kartu']):
        aspects.append('wasit')
    if 'pssi' in text_lower:
        aspects.append('PSSI')
    if 'federasi' in text_lower:
        aspects.append('federasi')
    if any(w in text_lower for w in ['fans', 'supporter', 'pendukung', 'dukungan', 'stadion']):
        aspects.append('fanbase')
    
    return aspects if aspects else ['pemain']

df['aspects'] = df['text'].apply(label_aspects)

# TOXICITY labeling
TOXIC_WORDS = {'bodoh', 'payah', 'tolol', 'sampah', 'goblok', 'berantakan', 'kacau'}

def label_toxicity(text):
    """Binary toxicity classification"""
    text_lower = text.lower()
    toxic_count = sum(1 for w in TOXIC_WORDS if w in text_lower)
    is_toxic = toxic_count > 0
    score = min(0.9, 0.5 + toxic_count * 0.2) if is_toxic else max(0.1, 0.3 - len(text.split()) * 0.01)
    return 'toxic' if is_toxic else 'non-toxic', score

df['toxicity_label'], df['toxicity_score'] = zip(*df['text'].apply(label_toxicity))

# STANCE labeling
stance_map = {'positif': 'pro', 'negatif': 'kontra', 'netral': 'tidak_jelas'}
df['stance'] = df['sentiment'].map(stance_map)

# INTENT labeling
def label_intent(text, text_original, sentiment):
    """Intent classification"""
    text_lower = text.lower()
    
    if '?' in text_original or any(w in text_lower for w in ['kapan', 'kenapa', 'mengapa', 'apakah', 'bagaimana', 'siapa']):
        return 'pertanyaan'
    elif sentiment == 'negatif' and any(w in text_lower for w in ['harus', 'kurang', 'buruk', 'gagal', 'tidak']):
        return 'komplain'
    elif any(w in text_lower for w in ['evaluasi', 'perlu', 'sebaiknya', 'harus benahi', 'tingkatkan']):
        return 'saran'
    elif any(w in text_lower for w in ['dukung', 'ayo', 'tetap', 'mari', 'yuk', 'semangat', 'terus']):
        return 'ajakan'
    elif any(w in text_lower for w in ['haha', 'wkwk', 'lucu', 'kocak', 'ngakak']):
        return 'humor'
    else:
        return 'informasi'

df['intent'] = df.apply(lambda row: label_intent(row['text_cleaned'], row['text'], row['sentiment']), axis=1)

print(f"‚úÖ All layers labeled!")
print(f"\nüìä Layer distributions:")
print(f"   Emotions: {sum(len(e) for e in df['emotions'])} labels across {len(df)} comments")
print(f"   Aspects: {sum(len(a) for a in df['aspects'])} labels across {len(df)} comments")
print(f"   Toxic: {(df['toxicity_label'] == 'toxic').sum()} ({(df['toxicity_label'] == 'toxic').sum()/len(df)*100:.1f}%)")
print(f"   Intents: {df['intent'].value_counts().to_dict()}")

# ============================================================================
# STEP 5: CREATE COMPREHENSIVE OUTPUT
# ============================================================================
print("\nüì¶ STEP 5: Creating comprehensive output...")

# Expand multi-label columns for output
output_df = df.copy()
output_df['emotions_str'] = output_df['emotions'].apply(lambda x: ','.join(x))
output_df['aspects_str'] = output_df['aspects'].apply(lambda x: ','.join(x))
df['low_confidence_flag'] = df['sentiment_confidence'] < 0.6
output_df['low_confidence_flag'] = output_df['sentiment_confidence'] < 0.6

# Select final columns
final_cols = [
    'comment_id', 'text', 'text_cleaned', 'sentiment', 'sentiment_confidence',
    'emotions_str', 'aspects_str', 'toxicity_label', 'toxicity_score',
    'stance', 'intent', 'likes', 'low_confidence_flag'
]

output_df_final = output_df[final_cols].copy()
output_df_final.columns = [
    'comment_id', 'text', 'text_cleaned', 'sentiment', 'sentiment_confidence',
    'emotions', 'aspects', 'toxicity_label', 'toxicity_score',
    'stance', 'intent', 'like_count', 'low_confidence_flag'
]

# Save processed data
os.makedirs('data/processed', exist_ok=True)
output_df_final.to_csv('data/processed/layered_predictions_FULL.csv', index=False, encoding='utf-8')
df.to_csv('data/processed/comments_processed_FULL.csv', index=False, encoding='utf-8')

print(f"‚úÖ Output created: {output_df_final.shape}")
print(f"   üíæ Saved: data/processed/layered_predictions_FULL.csv")
print(f"   üíæ Saved: data/processed/comments_processed_FULL.csv")

# ============================================================================
# STEP 6: COMPREHENSIVE ANALYTICS & REPORT
# ============================================================================
print("\nüìä STEP 6: Generating comprehensive analytics...")

report_lines = []
report_lines.append("="*100)
report_lines.append("üéØ MULTI-LAYER ANALYTICS - COMPREHENSIVE REPORT")
report_lines.append("="*100)
report_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
report_lines.append(f"Dataset: {len(df)} comments across {len(df['video_id'].unique())} videos")
report_lines.append("="*100)

# 1. SENTIMENT OVERVIEW
report_lines.append("\nüìä 1. SENTIMENT ANALYSIS")
report_lines.append("-" * 100)
for sent in ['positif', 'netral', 'negatif']:
    count = (df['sentiment'] == sent).sum()
    pct = count / len(df) * 100
    avg_conf = df[df['sentiment'] == sent]['sentiment_confidence'].mean()
    report_lines.append(f"   {sent.upper():10} : {count:3} comments ({pct:5.1f}%) | Avg Confidence: {avg_conf:.3f}")

low_conf = df['low_confidence_flag'].sum()
report_lines.append(f"\n   Low confidence predictions: {low_conf} ({low_conf/len(df)*100:.1f}%)")

# 2. EMOTION BREAKDOWN
report_lines.append("\n\nüòä 2. EMOTION ANALYSIS (Multi-Label)")
report_lines.append("-" * 100)
all_emotions = [e for emotions in df['emotions'] for e in emotions]
emotion_counts = Counter(all_emotions)
total_labels = len(all_emotions)
for emo, count in emotion_counts.most_common():
    pct = count / len(df) * 100
    report_lines.append(f"   {emo:15} : {count:3} mentions ({pct:5.1f}% of comments)")

report_lines.append(f"\n   Total emotion labels: {total_labels} (Avg {total_labels/len(df):.2f} per comment)")

# 3. ASPECT ANALYSIS
report_lines.append("\n\nüéØ 3. ASPECT ANALYSIS (Multi-Label)")
report_lines.append("-" * 100)
all_aspects = [a for aspects in df['aspects'] for a in aspects]
aspect_counts = Counter(all_aspects)
total_asp_labels = len(all_aspects)
for asp, count in aspect_counts.most_common():
    pct = count / len(df) * 100
    report_lines.append(f"   {asp:15} : {count:3} mentions ({pct:5.1f}% of comments)")

report_lines.append(f"\n   Total aspect labels: {total_asp_labels} (Avg {total_asp_labels/len(df):.2f} per comment)")

# 4. TOXICITY
report_lines.append("\n\n‚ö†Ô∏è  4. TOXICITY ANALYSIS")
report_lines.append("-" * 100)
toxic_count = (df['toxicity_label'] == 'toxic').sum()
non_toxic = (df['toxicity_label'] == 'non-toxic').sum()
report_lines.append(f"   TOXIC        : {toxic_count:3} comments ({toxic_count/len(df)*100:5.1f}%)")
report_lines.append(f"   NON-TOXIC    : {non_toxic:3} comments ({non_toxic/len(df)*100:5.1f}%)")
report_lines.append(f"   Avg toxicity score: {df['toxicity_score'].mean():.3f}")

# Top toxic comments
toxic_df = df[df['toxicity_label'] == 'toxic'].nlargest(3, 'likes')
if len(toxic_df) > 0:
    report_lines.append(f"\n   ‚ö° Top toxic comments (by likes):")
    for idx, row in toxic_df.iterrows():
        report_lines.append(f"      ‚Ä¢ \"{row['text'][:70]}...\"")
        report_lines.append(f"        Likes: {row['likes']} | Score: {row['toxicity_score']:.2f} | Aspects: {','.join(row['aspects'])}")

# 5. STANCE
report_lines.append("\n\nüîÄ 5. STANCE ANALYSIS")
report_lines.append("-" * 100)
for stance in ['pro', 'kontra', 'tidak_jelas']:
    count = (df['stance'] == stance).sum()
    pct = count / len(df) * 100
    report_lines.append(f"   {stance.upper():15} : {count:3} comments ({pct:5.1f}%)")

# 6. INTENT
report_lines.append("\n\nüí¨ 6. INTENT ANALYSIS")
report_lines.append("-" * 100)
intent_counts = df['intent'].value_counts()
for intent, count in intent_counts.items():
    pct = count / len(df) * 100
    report_lines.append(f"   {intent:15} : {count:3} comments ({pct:5.1f}%)")

# 7. CROSS-ANALYSIS
report_lines.append("\n\nüîç 7. CROSS-LAYER INSIGHTS")
report_lines.append("-" * 100)

# Negative aspects
neg_df = df[df['sentiment'] == 'negatif']
neg_aspects = [a for aspects in neg_df['aspects'] for a in aspects]
neg_aspect_counts = Counter(neg_aspects)
report_lines.append("\n   üìâ Most criticized aspects:")
for asp, count in neg_aspect_counts.most_common(5):
    report_lines.append(f"      ‚Ä¢ {asp}: {count} negative mentions")

# Positive aspects
pos_df = df[df['sentiment'] == 'positif']
pos_aspects = [a for aspects in pos_df['aspects'] for a in aspects]
pos_aspect_counts = Counter(pos_aspects)
report_lines.append("\n   üìà Most praised aspects:")
for asp, count in pos_aspect_counts.most_common(5):
    report_lines.append(f"      ‚Ä¢ {asp}: {count} positive mentions")

# High engagement
high_engagement = df.nlargest(5, 'likes')
report_lines.append("\n   üî• Highest engagement comments:")
for idx, row in high_engagement.iterrows():
    report_lines.append(f"      ‚Ä¢ {row['likes']} likes | {row['sentiment']} | {','.join(row['emotions'])}")
    report_lines.append(f"        \"{row['text'][:70]}...\"")

# 8. ACTIONABLE RECOMMENDATIONS
report_lines.append("\n\nüí° 8. ACTIONABLE RECOMMENDATIONS")
report_lines.append("-" * 100)

report_lines.append("\n   üéØ PRIORITY ACTIONS:")
report_lines.append(f"      1. Address {neg_df.shape[0]} negative comments ({neg_df.shape[0]/len(df)*100:.1f}%)")
report_lines.append(f"         Top concerns: {', '.join([a for a, c in neg_aspect_counts.most_common(3)])}")

questions = df[df['intent'] == 'pertanyaan']
report_lines.append(f"\n      2. Respond to {len(questions)} questions from fans")

complaints = df[df['intent'] == 'komplain']
constructive = complaints[complaints['toxicity_label'] == 'non-toxic']
report_lines.append(f"\n      3. Review {len(constructive)} constructive complaints")
report_lines.append(f"         (Filter out {toxic_count} toxic comments)")

report_lines.append("\n   üì¢ COMMUNICATION STRATEGY:")
report_lines.append(f"      ‚Ä¢ Emphasize positive aspects: {', '.join([a for a, c in pos_aspect_counts.most_common(3)])}")
report_lines.append(f"      ‚Ä¢ Engage with {len(questions)} questions proactively")
report_lines.append(f"      ‚Ä¢ Moderate {toxic_count} toxic comments")

ajakan = df[df['intent'] == 'ajakan']
report_lines.append(f"\n   ü§ù COMMUNITY ENGAGEMENT:")
report_lines.append(f"      ‚Ä¢ {len(ajakan)} supportive 'ajakan' posts - amplify these!")
report_lines.append(f"      ‚Ä¢ {len(pos_df)} positive comments - showcase fan loyalty")

report_lines.append("\n   ‚ö° CRISIS INDICATORS:")
toxic_pct = toxic_count / len(df) * 100
neg_pct = len(neg_df) / len(df) * 100
if toxic_pct > 10:
    report_lines.append(f"      ‚ö†Ô∏è  HIGH TOXICITY: {toxic_pct:.1f}% toxic comments")
if neg_pct > 50:
    report_lines.append(f"      ‚ö†Ô∏è  NEGATIVE SENTIMENT MAJORITY: {neg_pct:.1f}% negative")
else:
    report_lines.append(f"      ‚úÖ Sentiment levels manageable")

report_lines.append("\n" + "="*100)
report_lines.append("END OF REPORT")
report_lines.append("="*100)

# Save report
os.makedirs('results/reports', exist_ok=True)
report_path = 'results/reports/FULL_ANALYTICS_REPORT.txt'
with open(report_path, 'w', encoding='utf-8') as f:
    f.write('\n'.join(report_lines))

print(f"‚úÖ Report generated: {report_path}")

# Print report to console
print("\n" + '\n'.join(report_lines))

# ============================================================================
# STEP 7: SAVE METRICS JSON
# ============================================================================
print("\nüíæ STEP 7: Saving metrics...")

metrics = {
    "dataset": {
        "total_comments": len(df),
        "total_videos": len(df['video_id'].unique()),
        "avg_text_length": float(df['text_length'].mean()),
        "avg_word_count": float(df['word_count'].mean()),
    },
    "sentiment": {
        "positif": int((df['sentiment'] == 'positif').sum()),
        "netral": int((df['sentiment'] == 'netral').sum()),
        "negatif": int((df['sentiment'] == 'negatif').sum()),
        "avg_confidence": float(df['sentiment_confidence'].mean()),
        "low_confidence_count": int(df['low_confidence_flag'].sum()),
    },
    "emotions": dict(emotion_counts),
    "aspects": dict(aspect_counts),
    "toxicity": {
        "toxic": int(toxic_count),
        "non_toxic": int(non_toxic),
        "avg_score": float(df['toxicity_score'].mean()),
    },
    "stance": {k: int(v) for k, v in df['stance'].value_counts().items()},
    "intent": {k: int(v) for k, v in intent_counts.items()},
    "top_negative_aspects": {k: int(v) for k, v in dict(neg_aspect_counts.most_common(5)).items()},
    "top_positive_aspects": {k: int(v) for k, v in dict(pos_aspect_counts.most_common(5)).items()},
}

metrics_path = 'results/reports/metrics_FULL.json'
with open(metrics_path, 'w', encoding='utf-8') as f:
    json.dump(metrics, f, indent=2, ensure_ascii=False, default=int)

print(f"‚úÖ Metrics saved: {metrics_path}")

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n\n" + "="*100)
print("üéâ FULL PIPELINE COMPLETED SUCCESSFULLY!")
print("="*100)

print(f"\nüì¶ DELIVERABLES:")
print(f"   ‚úÖ Raw data          : data/raw/comments_full.csv")
print(f"   ‚úÖ Processed data    : data/processed/comments_processed_FULL.csv")
print(f"   ‚úÖ Predictions       : data/processed/layered_predictions_FULL.csv")
print(f"   ‚úÖ Report            : results/reports/FULL_ANALYTICS_REPORT.txt")
print(f"   ‚úÖ Metrics JSON      : results/reports/metrics_FULL.json")

print(f"\nüìä QUICK STATS:")
print(f"   Total Comments  : {len(df)}")
print(f"   Positif         : {(df['sentiment'] == 'positif').sum()} ({(df['sentiment'] == 'positif').sum()/len(df)*100:.1f}%)")
print(f"   Negatif         : {(df['sentiment'] == 'negatif').sum()} ({(df['sentiment'] == 'negatif').sum()/len(df)*100:.1f}%)")
print(f"   Netral          : {(df['sentiment'] == 'netral').sum()} ({(df['sentiment'] == 'netral').sum()/len(df)*100:.1f}%)")
print(f"   Toxic           : {toxic_count} ({toxic_count/len(df)*100:.1f}%)")
print(f"   Emotion labels  : {len(all_emotions)} total")
print(f"   Aspect labels   : {len(all_aspects)} total")

print(f"\nüéØ TOP INSIGHTS:")
print(f"   Most discussed aspect    : {aspect_counts.most_common(1)[0][0]}")
print(f"   Most common emotion      : {emotion_counts.most_common(1)[0][0]}")
print(f"   Most criticized aspect   : {neg_aspect_counts.most_common(1)[0][0]}")
print(f"   Most praised aspect      : {pos_aspect_counts.most_common(1)[0][0]}")
print(f"   Dominant intent          : {intent_counts.idxmax()}")

print(f"\nüí° NEXT STEPS:")
print(f"   1. Review comprehensive report: results/reports/FULL_ANALYTICS_REPORT.txt")
print(f"   2. Analyze predictions CSV: data/processed/layered_predictions_FULL.csv")
print(f"   3. Create visualizations (optional)")
print(f"   4. Build real-time dashboard")
print(f"   5. For production: Add real YouTube API scraping")

print("\n" + "="*100)
print(f"‚ú® Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*100)
print("\nüöÄ FULL COMPLETE PIPELINE - ALL DONE!")
print("="*100)
